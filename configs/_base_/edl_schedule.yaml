train_cfg:
    do_train: true
    do_eval: true
    eval_strategy: 'epoch'
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    # lr 5e-5
    learning_rate: 0.00005
    # wd 1e-3
    weight_decay: 0.001
    # gradient clipping is important for EDL
    max_grad_norm: 20.0
    max_steps: 10080
    lr_scheduler_type: 'cosine'
    warmup_steps: 20
    logging_steps: 20
    save_strategy: 'epoch'
    save_total_limit: 1
    save_only_model: true
    bf16: true
