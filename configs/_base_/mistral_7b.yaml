model:
    model_name_or_path: "mistralai/Mistral-7B-v0.1"
    peft_path: null
    model_cfg:
        type: AutoModelForCausalLM
        torch_dtype: bfloat16
#        attn_implementation: "flash_attention_2"
    tokenizer_cfg:
        type: AutoTokenizer
        use_fast: true
    special_tokens: {}
    use_peft: True
    peft_cfg:
        type: LoraConfig
        r: 8
        lora_alpha: 16
        lora_dropout: 0.1
        task_type: "CAUSAL_LM"
        inference_mode: false
        bias: lora_only
        target_modules: ["q_proj", "v_proj", "lm_head"]

tokenizer_run_cfg:
    padding: true
    truncation: true
    return_tensors: "pt"
    max_length: 300
